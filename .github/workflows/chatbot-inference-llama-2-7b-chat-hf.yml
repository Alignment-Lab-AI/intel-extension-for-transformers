name: Chatbot inference on llama-2-7b-chat-hf

on:
  workflow_call:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-inf-lla-7b
  cancel-in-progress: true
  
env:
    REPO_DIR: "/intel-extension-for-transformers"
    REPO_TAG: "latest"
    DOCKER_FILE_NAME: "intel_extension_for_transformers/neural_chat/docker/Dockerfile"
    CONTAINER_NAME: "chatbotinfer"
    EXTRA_CONTAINER_NAME: "chatbotfinetune-mpi"

jobs:
  inference:
    name: inference test
    runs-on: itrex-node
    steps:
      - name: Docker Clean Up
        run: |
          docker ps -a
          if [[ $(docker ps -a | grep -i '${{ env.CONTAINER_NAME }}'$) ]]; then
              docker start ${{ env.CONTAINER_NAME }}
              echo "remove left files through container ..."
              docker exec ${{ env.CONTAINER_NAME }} bash -c "ls -a ${{ env.REPO_DIR }} && rm -fr ${{ env.REPO_DIR }}/* && rm -fr ${{ env.REPO_DIR }}/.* || true"
          fi
          if [[ $(docker ps -a | grep -i '${{ env.EXTRA_CONTAINER_NAME }}') ]]; then
              docker start ${{ env.EXTRA_CONTAINER_NAME }}
              echo "remove left files through container ..."
              docker exec ${{ env.EXTRA_CONTAINER_NAME }} bash -c "ls -a ${{ env.REPO_DIR }} && rm -fr ${{ env.REPO_DIR }}/* && rm -fr ${{ env.REPO_DIR }}/.* || true"
          fi
      - name: Checkout
        uses: actions/checkout@v2

      - name: Load environment variables
        run: cat ~/itrex-actions-runner/.env >> $GITHUB_ENV
      

      - name: Build Docker Image
        run:           
          docker build --no-cache ./ --target cpu --build-arg ITREX_VER=${{ github.sha }} -f ${{ env.DOCKER_FILE_NAME }} -t ${{ env.CONTAINER_NAME }}:${{ env.REPO_TAG }} && yes | docker container prune && yes | docker image prune

      - name: Start Docker Container
        run: |
          cid=$(docker ps -q --filter "name=${{ env.CONTAINER_NAME }}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          docker run -tid -v ${{ env.CACHE_DIR }}:/root/.cache -v .:/root/chatbot --name=${{ env.CONTAINER_NAME }} --hostname="chatbotinfer-container" ${{ env.CONTAINER_NAME }}:${{ env.REPO_TAG }}

      - name: Run Inference Test
        run: |
          docker exec ${{ env.CONTAINER_NAME }} bash -c "cd /root/chatbot && source activate && conda activate neuralchat; python workflows/chatbot/inference/generate.py --base_model_path \"meta-llama/Llama-2-7b-chat-hf\" --hf_access_token \"${{ env.HF_ACCESS_TOKEN }}\" --instructions \"Transform the following sentence into one that shows contrast. The tree is rotten.\" "

      - name: Stop Container
        if: success() || failure()
        run: |
          cid=$(docker ps -q --filter "name=${{ env.CONTAINER_NAME }}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "Inference completed successfully"
