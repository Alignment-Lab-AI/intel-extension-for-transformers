<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Runtime &mdash; Intel® Extension for Transformers 0.1.dev1+g40eff8f documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LLM Runtime</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/docs/intel_extension_for_transformers/llm/runtime/graph/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-runtime">
<h1>LLM Runtime<a class="headerlink" href="#llm-runtime" title="Link to this heading"></a></h1>
<p>LLM Runtime is designed to provide the efficient inference of large language models (LLMs) on Intel platforms through the state-of-the-art (SOTA) model compression techniques. The work is highly inspired from <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, which organizes almost all the core code (e.g., kernels) in a single big file with a large number of pre-defined macros, thus making it not easy for developers to support a new model. Our LLM Runtime has the following features:</p>
<ul class="simple">
<li><p>Modular design to support new models</p></li>
<li><p>Highly optimized low precision kernels</p></li>
<li><p>Utilize AMX, VNNI and AVX512F instruction set</p></li>
<li><p>Support CPU (x86 platforms only) and initial (Intel) GPU</p></li>
<li><p>Support 4bits and 8bits quantization</p></li>
</ul>
<blockquote>
<div><p>LLM Runtime is under active development so APIs are subject to change.</p>
</div></blockquote>
<section id="supported-models">
<h2>Supported Models<a class="headerlink" href="#supported-models" title="Link to this heading"></a></h2>
<p>We support the following models:</p>
<section id="text-generation-models">
<h3>Text generation models<a class="headerlink" href="#text-generation-models" title="Link to this heading"></a></h3>
<p>| model name | INT8 | INT4|
|—|:—:|:—:|
|<a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">LLaMA2-7B</a>, <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf">LLaMA2-13B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/decapoda-research/llama-7b-hf">LLaMA-7B</a>, <a class="reference external" href="https://huggingface.co/decapoda-research/llama-13b-hf">LLaMA-13B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/EleutherAI/gpt-j-6b">GPT-J-6B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/EleutherAI/gpt-neox-20b">GPT-NeoX-20B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/databricks/dolly-v2-3b">Dolly-v2-3B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/mosaicml/mpt-7b">MPT-7B</a>, <a class="reference external" href="https://huggingface.co/mosaicml/mpt-30b">MPT-30B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/tiiuae/falcon-7b">Falcon-7B</a>, <a class="reference external" href="https://huggingface.co/tiiuae/falcon-40b">Falcon-40B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/bigscience/bloomz-7b1">BLOOM-7B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/facebook/opt-125m">OPT-125m</a>, <a class="reference external" href="https://huggingface.co/facebook/opt-350m">OPT-350m</a>, <a class="reference external" href="https://huggingface.co/facebook/opt-1.3b">OPT-1.3B</a>, <a class="reference external" href="https://huggingface.co/facebook/opt-13b">OPT-13B</a>| ✅ | ✅ |<br />|<a class="reference external" href="https://huggingface.co/THUDM/chatglm-6b">ChatGLM-6B</a>, <a class="reference external" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a>| ✅ | ✅ |</p>
</section>
<section id="code-generation-models">
<h3>Code generation models<a class="headerlink" href="#code-generation-models" title="Link to this heading"></a></h3>
<p>| model name | INT8 | INT4|
|—|:—:|:—:|
|<a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf">Code-LLaMA-7B</a>, <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-13b-hf">Code-LLaMA-13B</a>| ✅ | ✅ |
|<a class="reference external" href="https://huggingface.co/bigcode/starcoderbase-1b">StarCoder-1B</a>, <a class="reference external" href="https://huggingface.co/bigcode/starcoderbase-3b">StarCoder-3B</a>, <a class="reference external" href="https://huggingface.co/bigcode/starcoder">StarCoder-15.5B</a>| ✅ | ✅ |</p>
</section>
</section>
<section id="how-to-use">
<h2>How to use<a class="headerlink" href="#how-to-use" title="Link to this heading"></a></h2>
<section id="build-llm-runtime">
<h3>1. Build LLM Runtime<a class="headerlink" href="#build-llm-runtime" title="Link to this heading"></a></h3>
<p>Linux</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build
<span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>..<span class="w"> </span>-G<span class="w"> </span>Ninja
ninja
</pre></div>
</div>
<p>Windows: install VisualStudio 2022(a validated veresion), search ‘Developer PowerShell for VS 2022’ and open it, then run the following .htmls.</p>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="n">build</span>
<span class="nb">cd </span><span class="n">build</span>
<span class="n">cmake</span> <span class="p">..</span>
<span class="n">cmake</span> <span class="p">-</span><span class="n">-build</span> <span class="p">.</span> <span class="n">-j</span>
</pre></div>
</div>
</section>
<section id="run-llm-with-python-api">
<h3>2. Run LLM with Python API<a class="headerlink" href="#run-llm-with-python-api" title="Link to this heading"></a></h3>
<p>You can use the python api to simplely run HF model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">WeightOnlyQuantConfig</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/gpt-j-6b&quot;</span>     <span class="c1"># support model id of HF or local PATH to model</span>
<span class="n">woq_config</span> <span class="o">=</span> <span class="n">WeightOnlyQuantConfig</span><span class="p">(</span><span class="n">compute_dtype</span><span class="o">=</span><span class="s2">&quot;int8&quot;</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="o">=</span><span class="s2">&quot;int4&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">woq_config</span><span class="p">,</span> <span class="n">use_llm_runtime</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Once upon a time, a little girl&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-llm-with-script">
<h3>3. Run LLM with Script<a class="headerlink" href="#run-llm-with-script" title="Link to this heading"></a></h3>
<p>You can use the following script to run, including convertion, quantization and inference.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">python</span><span class="w"> </span><span class="n">scripts</span><span class="o">/</span><span class="n">run</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">model</span><span class="o">-</span><span class="n">path</span><span class="w"> </span><span class="o">--</span><span class="n">weight_dtype</span><span class="w"> </span><span class="n">int4</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="s">&quot;She opened the door and see&quot;</span>
</pre></div>
</div>
<p>LLM one-click running script args explanations:
| arg               | explanation                                                             |
| ————–    | ———————————————————————– |
| model           | directory containing model file or model id                               |
| –weight_dtype  | data type of quantized weight (default: int4)                             |
| –alg           | quantization algorithm to use: sym/asym (default: sym)                    |
| –group_size    | group size (default: 32)                                                  |
| –scale_dtype   | fp32/bf16 type for scales (default: fp32)                                 |
| –compute_dtype | data type of Gemm computation: int8/bf16/fp32 (default: int8)             |
| –use_ggml      | enable ggml for quantization and inference                                |
| -p / –prompt     | prompt to start generation with (default: empty)                        |
| -n / –n_predict  | number of tokens to predict (default: -1, -1 = infinity)                |
| -t / –threads    | number of threads to use during computation (default: 56)               |
| -b / –batch_size_truncate | batch size for prompt processing (default: 512)                |
| -c / –ctx_size   | size of the prompt context (default: 512, can not be larger than specific model’s context window length)                                                                                       |
| -s / –seed       | NG seed (default: -1, use random seed for &lt; 0)                          |
| –repeat_penalty  | penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)       |
| –color           | colorise output to distinguish prompt and user input from generations   |
| –keep            | number of tokens to keep from the initial prompt (default: 0, -1 = all) |</p>
</section>
</section>
<section id="advanced-use">
<h2>Advanced use<a class="headerlink" href="#advanced-use" title="Link to this heading"></a></h2>
<section id="convert-and-quantize-llm-model">
<h3>1. Convert and Quantize LLM model<a class="headerlink" href="#convert-and-quantize-llm-model" title="Link to this heading"></a></h3>
<p>LLM Runtime assumes the same model format as <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> and <a class="reference external" href="https://github.com/ggerganov/ggml">ggml</a>. You can also convert the model by following the below steps:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># convert the model directly use model id in Hugging Face. (recommended)</span>
python<span class="w"> </span>scripts/convert.py<span class="w"> </span>--outtype<span class="w"> </span>f32<span class="w"> </span>--outfile<span class="w"> </span>ne-f32.bin<span class="w"> </span>EleutherAI/gpt-j-6b

<span class="c1"># or you can download fp32 model (e.g., LLAMA2) from Hugging Face at first, then convert the pytorch model to ggml format.</span>
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
python<span class="w"> </span>scripts/convert.py<span class="w"> </span>--outtype<span class="w"> </span>f32<span class="w"> </span>--outfile<span class="w"> </span>ne-f32.bin<span class="w"> </span>model_path

<span class="c1"># quantize weights of fp32 ggml bin</span>
<span class="c1"># model_name: llama, llama2, mpt, falcon, gptj, starcoder, dolly</span>
<span class="c1"># optimized INT4 model with group size 128 (recommended)</span>
python<span class="w"> </span>scripts/quantize.py<span class="w"> </span>--model_name<span class="w"> </span>llama2<span class="w"> </span>--model_file<span class="w"> </span>ne-f32.bin<span class="w"> </span>--out_file<span class="w"> </span>ne-q4_j.bin<span class="w"> </span>--weight_dtype<span class="w"> </span>int4<span class="w"> </span>--group_size<span class="w"> </span><span class="m">128</span><span class="w"> </span>--compute_dtype<span class="w"> </span>int8

<span class="c1"># Alternativly you could run ggml q4_0 format like following</span>
python<span class="w"> </span>scripts/quantize.py<span class="w"> </span>--model_name<span class="w"> </span>llama2<span class="w"> </span>--model_file<span class="w"> </span>ne-f32.bin<span class="w"> </span>--out_file<span class="w"> </span>ne-q4_0.bin<span class="w"> </span>--weight_dtype<span class="w"> </span>int4<span class="w"> </span>--use_ggml
<span class="c1"># optimized INT4 model with group size 32</span>
python<span class="w"> </span>scripts/quantize.py<span class="w"> </span>--model_name<span class="w"> </span>llama2<span class="w"> </span>--model_file<span class="w"> </span>ne-f32.bin<span class="w"> </span>--out_file<span class="w"> </span>ne-q4_j.bin<span class="w"> </span>--weight_dtype<span class="w"> </span>int4<span class="w"> </span>--group_size<span class="w"> </span><span class="m">32</span><span class="w"> </span>--compute_dtype<span class="w"> </span>int8
</pre></div>
</div>
<p>quantization args explanations:
| arg             | explanation                                                 |
| ————–  | ———————————————————– |
| –model_file    | path to the fp32 model                                      |
| –out_file      | path to the quantized model                                 |
| –config        | path to the configuration file (default: “”)                |
| –nthread       | number of threads to use (default: 1)                       |
| –weight_dtype  | data type of quantized weight: int4/int8 (default: int4)    |
| –alg           | quantization algorithm to use: sym/asym (default: sym)      |
| –group_size    | group size (default: 32)                                    |
| –scale_dtype   | data type of scales: bf16/fp32 (default: fp32)              |
| –compute_dtype | data type of Gemm computation: int8/bf16/fp32 (default: int8)  |
| –use_ggml      | enable ggml for quantization and inference                  |</p>
</section>
<section id="inference-model-with-c-script-api">
<h3>2. Inference model with C++ script API<a class="headerlink" href="#inference-model-with-c-script-api" title="Link to this heading"></a></h3>
<p>We supply LLM running script to run supported models with c++ api conveniently.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># recommed to use numactl to bind cores in Intel cpus for better performance</span>
<span class="c1"># if you use different core numbers, please also  change -t arg value</span>
<span class="c1"># please type prompt about codes when run `StarCoder`, for example, -p &quot;def fibonnaci(&quot;.</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span><span class="m">0</span><span class="w"> </span>-C<span class="w"> </span><span class="m">0</span>-55<span class="w"> </span>python<span class="w"> </span>scripts/inference.py<span class="w"> </span>--model_name<span class="w"> </span>llama<span class="w"> </span>-m<span class="w"> </span>ne-q4_j.bin<span class="w"> </span>-c<span class="w"> </span><span class="m">512</span><span class="w"> </span>-b<span class="w"> </span><span class="m">1024</span><span class="w"> </span>-n<span class="w"> </span><span class="m">256</span><span class="w"> </span>-t<span class="w"> </span><span class="m">56</span><span class="w"> </span>--color<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;She opened the door and see&quot;</span>

<span class="c1"># if you want to generate fixed outputs, please set --seed arg, for example:</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span><span class="m">0</span><span class="w"> </span>-C<span class="w"> </span><span class="m">0</span>-55<span class="w"> </span>python<span class="w"> </span>scripts/inference.py<span class="w"> </span>--model_name<span class="w"> </span>llama<span class="w"> </span>-m<span class="w"> </span>ne-q4_j.bin<span class="w"> </span>-c<span class="w"> </span><span class="m">512</span><span class="w"> </span>-b<span class="w"> </span><span class="m">1024</span><span class="w"> </span>-n<span class="w"> </span><span class="m">256</span><span class="w"> </span>-t<span class="w"> </span><span class="m">56</span><span class="w"> </span>--color<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;She opened the door and see&quot;</span><span class="w"> </span>--seed<span class="w"> </span><span class="m">12</span>

<span class="c1"># if you want to reduce repeated generated texts, please set --repeat_penalty (value &gt; 1.0, default = 1.0), for example:</span>
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>numactl<span class="w"> </span>-m<span class="w"> </span><span class="m">0</span><span class="w"> </span>-C<span class="w"> </span><span class="m">0</span>-55<span class="w"> </span>python<span class="w"> </span>scripts/inference.py<span class="w"> </span>--model_name<span class="w"> </span>llama<span class="w"> </span>-m<span class="w"> </span>ne-q4_j.bin<span class="w"> </span>-c<span class="w"> </span><span class="m">512</span><span class="w"> </span>-b<span class="w"> </span><span class="m">1024</span><span class="w"> </span>-n<span class="w"> </span><span class="m">256</span><span class="w"> </span>-t<span class="w"> </span><span class="m">56</span><span class="w"> </span>--color<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;She opened the door and see&quot;</span><span class="w"> </span>--repeat_penalty<span class="w"> </span><span class="m">1</span>.2
</pre></div>
</div>
<p>LLM running script args explanations:
| arg               | explanation                                                             |
| ————–    | ———————————————————————– |
| –model_name      | model name                                                              |
| -m / –model      | path to the executed model                                              |
| -p / –prompt     | prompt to start generation with (default: empty)                        |
| -n / –n_predict  | number of tokens to predict (default: -1, -1 = infinity)                |
| -t / –threads    | number of threads to use during computation (default: 56)               |
| -b / –batch_size | batch size for prompt processing (default: 512)                         |
| -c / –ctx_size   | size of the prompt context (default: 512, can not be larger than specific model’s context window length)                                                                                |
| -s / –seed       | NG seed (default: -1, use random seed for &lt; 0)                          |
| –repeat_penalty  | penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)       |
| –color           | colorise output to distinguish prompt and user input from generations   |
| –keep            | number of tokens to keep from the initial prompt (default: 0, -1 = all) |
| –glm_tokenizer   | the path of the chatglm tokenizer (default: THUDM/chatglm-6b)           |</p>
</section>
<section id="tensor-parallelism-cross-nodes-sockets">
<h3>3. Tensor Parallelism cross nodes/sockets<a class="headerlink" href="#tensor-parallelism-cross-nodes-sockets" title="Link to this heading"></a></h3>
<p>We support tensor parallelism strategy for distributed inference/training on multi-node and multi-socket.  You can refer to <a class="reference external" href="./tensor_parallelism.html">tensor_parallelism.html</a> to enable this feature.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f276fa480a0> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>