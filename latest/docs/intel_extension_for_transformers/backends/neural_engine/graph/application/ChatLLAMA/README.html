<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Table of Contents &mdash; Intel® Extension for Transformers 0.1.dev1+g9978df3 documentation</title>
      <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../legal.html">Legal Information</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Table of Contents</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/docs/intel_extension_for_transformers/backends/neural_engine/graph/application/ChatLLAMA/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <blockquote>
<div><p><a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md">Original</a> of this documents.</p>
</div></blockquote>
<section id="table-of-contents">
<h1>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#quick-start">Quick Start</a></p></li>
<li><p><a class="reference external" href="#common-options">Common Options</a></p></li>
<li><p><a class="reference external" href="#input-prompts">Input Prompts</a></p></li>
<li><p><a class="reference external" href="#interaction">Interaction</a></p></li>
<li><p><a class="reference external" href="#context-management">Context Management</a></p></li>
<li><p><a class="reference external" href="#generation-flags">Generation Flags</a></p></li>
<li><p><a class="reference external" href="#performance-tuning-and-memory-options">Performance Tuning and Memory Options</a></p></li>
<li><p><a class="reference external" href="#additional-options">Additional Options</a></p></li>
</ol>
</section>
<section id="quick-start">
<h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this heading"></a></h1>
<p>To get started right away, run the following command, making sure to use the correct path for the model you have:</p>
<section id="unix-based-systems-linux-macos-etc">
<h2>Unix-based systems (Linux, macOS, etc.):<a class="headerlink" href="#unix-based-systems-linux-macos-etc" title="Permalink to this heading"></a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/main_llama<span class="w"> </span>-m<span class="w"> </span><span class="si">${</span><span class="nv">path_to_models</span><span class="si">}</span><span class="w"> </span>--prompt<span class="w"> </span><span class="s2">&quot;Once upon a time&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="common-options">
<h1>Common Options<a class="headerlink" href="#common-options" title="Permalink to this heading"></a></h1>
<p>In this section, we cover the most commonly used options for running the <code class="docutils literal notranslate"><span class="pre">main</span></code> program with the LLaMA models:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">FNAME,</span> <span class="pre">--model</span> <span class="pre">FNAME</span></code>: Specify the path to the LLaMA model file (e.g., <code class="docutils literal notranslate"><span class="pre">models/7B/ggml-model.bin</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-i,</span> <span class="pre">--interactive</span></code>: Run the program in interactive mode, allowing you to provide input directly and receive real-time responses.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-ins,</span> <span class="pre">--instruct</span></code>: Run the program in instruction mode, which is particularly useful when working with Alpaca models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">N,</span> <span class="pre">--n_predict</span> <span class="pre">N</span></code>: Set the number of tokens to predict when generating text. Adjusting this value can influence the length of the generated text.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">N,</span> <span class="pre">--ctx_size</span> <span class="pre">N</span></code>: Set the size of the prompt context. The default is 512, but LLaMA models were built with a context of 2048, which will provide better results for longer input/inference.</p></li>
</ul>
</section>
<section id="input-prompts">
<h1>Input Prompts<a class="headerlink" href="#input-prompts" title="Permalink to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">main</span></code> program provides several ways to interact with the LLaMA models using input prompts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--prompt</span> <span class="pre">PROMPT</span></code>: Provide a prompt directly as a command-line option.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--file</span> <span class="pre">FNAME</span></code>: Provide a file containing a prompt or multiple prompts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--interactive-first</span></code>: Run the program in interactive mode and wait for input right away. (More on this below.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--random-prompt</span></code>: Start with a randomized prompt.</p></li>
</ul>
</section>
<section id="interaction">
<h1>Interaction<a class="headerlink" href="#interaction" title="Permalink to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">main</span></code> program offers a seamless way to interact with LLaMA models, allowing users to engage in real-time conversations or provide instructions for specific tasks. The interactive mode can be triggered using various options, including <code class="docutils literal notranslate"><span class="pre">--interactive</span></code>, <code class="docutils literal notranslate"><span class="pre">--interactive-first</span></code>, and <code class="docutils literal notranslate"><span class="pre">--instruct</span></code>.</p>
<p>In interactive mode, users can participate in text generation by injecting their input during the process. Users can press <code class="docutils literal notranslate"><span class="pre">Ctrl+C</span></code> at any time to interject and type their input, followed by pressing <code class="docutils literal notranslate"><span class="pre">Return</span></code> to submit it to the LLaMA model. To submit additional lines without finalizing input, users can end the current line with a backslash (<code class="docutils literal notranslate"><span class="pre">\</span></code>) and continue typing.</p>
<section id="interaction-options">
<h2>Interaction Options<a class="headerlink" href="#interaction-options" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-i,</span> <span class="pre">--interactive</span></code>: Run the program in interactive mode, allowing users to engage in real-time conversations or provide specific instructions to the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--interactive-first</span></code>: Run the program in interactive mode and immediately wait for user input before starting the text generation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-ins,</span> <span class="pre">--instruct</span></code>: Run the program in instruction mode, which is specifically designed to work with Alpaca models that excel in completing tasks based on user instructions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--color</span></code>: Enable colorized output to differentiate visually distinguishing between prompts, user input, and generated text.</p></li>
</ul>
<p>By understanding and utilizing these interaction options, you can create engaging and dynamic experiences with the LLaMA models, tailoring the text generation process to your specific needs.</p>
</section>
<section id="reverse-prompts">
<h2>Reverse Prompts<a class="headerlink" href="#reverse-prompts" title="Permalink to this heading"></a></h2>
<p>Reverse prompts are a powerful way to create a chat-like experience with a LLaMA model by pausing the text generation when specific text strings are encountered:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-r</span> <span class="pre">PROMPT,</span> <span class="pre">--reverse-prompt</span> <span class="pre">PROMPT</span></code>: Specify one or multiple reverse prompts to pause text generation and switch to interactive mode. For example, <code class="docutils literal notranslate"><span class="pre">-r</span> <span class="pre">&quot;User:&quot;</span></code> can be used to jump back into the conversation whenever it’s the user’s turn to speak. This helps create a more interactive and conversational experience. However, the reverse prompt doesn’t work when it ends with a space.</p></li>
</ul>
<p>To overcome this limitation, you can use the <code class="docutils literal notranslate"><span class="pre">--in-prefix</span></code> flag to add a space or any other characters after the reverse prompt.</p>
</section>
<section id="in-prefix">
<h2>In-Prefix<a class="headerlink" href="#in-prefix" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">--in-prefix</span></code> flag is used to add a prefix to your input, primarily, this is used to insert a space after the reverse prompt. Here’s an example of how to use the <code class="docutils literal notranslate"><span class="pre">--in-prefix</span></code> flag in conjunction with the <code class="docutils literal notranslate"><span class="pre">--reverse-prompt</span></code> flag:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>./main<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;User:&quot;</span><span class="w"> </span>--in-prefix<span class="w"> </span><span class="s2">&quot; &quot;</span>
</pre></div>
</div>
</section>
<section id="in-suffix">
<h2>In-Suffix<a class="headerlink" href="#in-suffix" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">--in-suffix</span></code> flag is used to add a suffix after your input. This is useful for adding an “Assistant:” prompt after the user’s input. It’s added after the new-line character (<code class="docutils literal notranslate"><span class="pre">\n</span></code>) that’s automatically added to the end of the user’s input. Here’s an example of how to use the <code class="docutils literal notranslate"><span class="pre">--in-suffix</span></code> flag in conjunction with the <code class="docutils literal notranslate"><span class="pre">--reverse-prompt</span></code> flag:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>./main<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;User:&quot;</span><span class="w"> </span>--in-prefix<span class="w"> </span><span class="s2">&quot; &quot;</span><span class="w"> </span>--in-suffix<span class="w"> </span><span class="s2">&quot;Assistant:&quot;</span>
</pre></div>
</div>
</section>
<section id="instruction-mode">
<h2>Instruction Mode<a class="headerlink" href="#instruction-mode" title="Permalink to this heading"></a></h2>
<p>Instruction mode is particularly useful when working with Alpaca models, which are designed to follow user instructions for specific tasks:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-ins,</span> <span class="pre">--instruct</span></code>: Enable instruction mode to leverage the capabilities of Alpaca models in completing tasks based on user-provided instructions.</p></li>
</ul>
<p>Technical detail: the user’s input is internally prefixed with the reverse prompt (or <code class="docutils literal notranslate"><span class="pre">###</span> <span class="pre">Instruction:</span></code> as the default), and followed by <code class="docutils literal notranslate"><span class="pre">###</span> <span class="pre">Response:</span></code> (except if you just press Return without any input, to keep generating a longer response).</p>
<p>By understanding and utilizing these interaction options, you can create engaging and dynamic experiences with the LLaMA models, tailoring the text generation process to your specific needs.</p>
</section>
</section>
<section id="context-management">
<h1>Context Management<a class="headerlink" href="#context-management" title="Permalink to this heading"></a></h1>
<p>During text generation, LLaMA models have a limited context size, which means they can only consider a certain number of tokens from the input and generated text. When the context fills up, the model resets internally, potentially losing some information from the beginning of the conversation or instructions. Context management options help maintain continuity and coherence in these situations.</p>
<section id="context-size">
<h2>Context Size<a class="headerlink" href="#context-size" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">--ctx_size</span></code> option allows you to set the size of the prompt context used by the LLaMA models during text generation. A larger context size helps the model to better comprehend and generate responses for longer input or conversations.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">N,</span> <span class="pre">--ctx_size</span> <span class="pre">N</span></code>: Set the size of the prompt context (default: 512). The LLaMA models were built with a context of 2048, which will yield the best results on longer input/inference. However, increasing the context size beyond 2048 may lead to unpredictable results.</p></li>
</ul>
</section>
<section id="keep-prompt">
<h2>Keep Prompt<a class="headerlink" href="#keep-prompt" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">--keep</span></code> option allows users to retain the original prompt when the model runs out of context, ensuring a connection to the initial instruction or conversation topic is maintained.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--keep</span> <span class="pre">N</span></code>: Specify the number of tokens from the initial prompt to retain when the model resets its internal context. By default, this value is set to 0 (meaning no tokens are kept). Use <code class="docutils literal notranslate"><span class="pre">-1</span></code> to retain all tokens from the initial prompt.</p></li>
</ul>
<p>By utilizing context management options like <code class="docutils literal notranslate"><span class="pre">--ctx_size</span></code> and <code class="docutils literal notranslate"><span class="pre">--keep</span></code>, you can maintain a more coherent and consistent interaction with the LLaMA models, ensuring that the generated text remains relevant to the original prompt or conversation.</p>
</section>
</section>
<section id="generation-flags">
<h1>Generation Flags<a class="headerlink" href="#generation-flags" title="Permalink to this heading"></a></h1>
<p>The following options allow you to control the text generation process and fine-tune the diversity, creativity, and quality of the generated text according to your needs. By adjusting these options and experimenting with different combinations of values, you can find the best settings for your specific use case.</p>
<section id="number-of-tokens-to-predict">
<h2>Number of Tokens to Predict<a class="headerlink" href="#number-of-tokens-to-predict" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">N,</span> <span class="pre">--n_predict</span> <span class="pre">N</span></code>: Set the number of tokens to predict when generating text (default: 128, -1 = infinity).</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">--n_predict</span></code> option controls the number of tokens the model generates in response to the input prompt. By adjusting this value, you can influence the length of the generated text. A higher value will result in longer text, while a lower value will produce shorter text. A value of -1 will cause text to be generated without limit.</p>
<p>It is important to note that the generated text may be shorter than the specified number of tokens if an End-of-Sequence (EOS) token or a reverse prompt is encountered. In interactive mode text generation will pause and control will be returned to the user. In non-interactive mode, the program will end. In both cases, the text generation may stop before reaching the specified <code class="docutils literal notranslate"><span class="pre">n_predict</span></code> value. If you want the model to keep going without ever producing End-of-Sequence on its own, you can use the <code class="docutils literal notranslate"><span class="pre">--ignore-eos</span></code> parameter.</p>
</section>
<section id="temperature">
<h2>Temperature<a class="headerlink" href="#temperature" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--temp</span> <span class="pre">N</span></code>: Adjust the randomness of the generated text (default: 0.8).</p></li>
</ul>
<p>Temperature is a hyperparameter that controls the randomness of the generated text. It affects the probability distribution of the model’s output tokens. A higher temperature (e.g., 1.5) makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the output more focused, deterministic, and conservative. The default value is 0.8, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--temp</span> <span class="pre">0.5</span></code></p>
</section>
<section id="repeat-penalty">
<h2>Repeat Penalty<a class="headerlink" href="#repeat-penalty" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--repeat_penalty</span> <span class="pre">N</span></code>: Control the repetition of token sequences in the generated text (default: 1.1).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--repeat_last_n</span> <span class="pre">N</span></code>: Last n tokens to consider for penalizing repetition (default: 64, 0 = disabled, -1 = ctx_size).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no-penalize-nl</span></code>: Disable penalization for newline tokens when applying the repeat penalty.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">repeat_penalty</span></code> option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1.1.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">repeat_last_n</span></code> option controls the number of tokens in the history to consider for penalizing repetition. A larger value will look further back in the generated text to prevent repetitions, while a smaller value will only consider recent tokens. A value of 0 disables the penalty, and a value of -1 sets the number of tokens considered equal to the context size (<code class="docutils literal notranslate"><span class="pre">ctx_size</span></code>).</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">--no-penalize-nl</span></code> option to disable newline penalization when applying the repeat penalty. This option is particularly useful for generating chat conversations, dialogues, code, poetry, or any text where newline tokens play a significant role in structure and formatting. Disabling newline penalization helps maintain the natural flow and intended formatting in these specific use cases.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--repeat_penalty</span> <span class="pre">1.15</span> <span class="pre">--repeat_last_n</span> <span class="pre">128</span> <span class="pre">--no-penalize-nl</span></code></p>
</section>
<section id="top-k-sampling">
<h2>Top-K Sampling<a class="headerlink" href="#top-k-sampling" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--top_k</span> <span class="pre">N</span></code>: Limit the next token selection to the K most probable tokens (default: 40).</p></li>
</ul>
<p>Top-k sampling is a text generation method that selects the next token only from the top k most likely tokens predicted by the model. It helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output. A higher value for top_k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--top_k</span> <span class="pre">30</span></code></p>
</section>
<section id="top-p-sampling">
<h2>Top-P Sampling<a class="headerlink" href="#top-p-sampling" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--top_p</span> <span class="pre">N</span></code>: Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.9).</p></li>
</ul>
<p>Top-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top_p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--top_p</span> <span class="pre">0.95</span></code></p>
</section>
<section id="tail-free-sampling-tfs">
<h2>Tail Free Sampling (TFS)<a class="headerlink" href="#tail-free-sampling-tfs" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--tfs</span> <span class="pre">N</span></code>: Enable tail free sampling with parameter z (default: 1.0, 1.0 = disabled).</p></li>
</ul>
<p>Tail free sampling (TFS) is a text generation technique that aims to reduce the impact of less likely tokens, which may be less relevant, less coherent, or nonsensical, on the output. The method adjusts the logits (token probabilities) by raising them to the power of the parameter z. A higher value of z (e.g., 2.0) will further suppress less likely tokens from the tail of the distribution, while a value of 1.0 disables the effect of TFS. By setting the parameter z, you can control how much the probabilities of less likely tokens are reduced.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--tfs</span> <span class="pre">2.0</span></code></p>
</section>
<section id="locally-typical-sampling">
<h2>Locally Typical Sampling<a class="headerlink" href="#locally-typical-sampling" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--typical</span> <span class="pre">N</span></code>: Enable locally typical sampling with parameter p (default: 1.0, 1.0 = disabled).</p></li>
</ul>
<p>Locally typical sampling promotes the generation of contextually coherent and diverse text by sampling tokens that are typical or expected based on the surrounding context. By setting the parameter p between 0 and 1, you can control the balance between producing text that is locally coherent and diverse. A value closer to 1 will promote more contextually coherent tokens, while a value closer to 0 will promote more diverse tokens. A value equal to 1 disables locally typical sampling.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--typical</span> <span class="pre">0.9</span></code></p>
</section>
<section id="mirostat-sampling">
<h2>Mirostat Sampling<a class="headerlink" href="#mirostat-sampling" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--mirostat</span> <span class="pre">N</span></code>: Enable Mirostat sampling, controlling perplexity during text generation (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mirostat_lr</span> <span class="pre">N</span></code>: Set the Mirostat learning rate, parameter eta (default: 0.1).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mirostat_ent</span> <span class="pre">N</span></code>: Set the Mirostat target entropy, parameter tau (default: 5.0).</p></li>
</ul>
<p>Mirostat is an algorithm that actively maintains the quality of generated text within a desired range during text generation. It aims to strike a balance between coherence and diversity, avoiding low-quality output caused by excessive repetition (boredom traps) or incoherence (confusion traps).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--mirostat_lr</span></code> option sets the Mirostat learning rate (eta). The learning rate influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. The default value is <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--mirostat_ent</span></code> option sets the Mirostat target entropy (tau), which represents the desired perplexity value for the generated text. Adjusting the target entropy allows you to control the balance between coherence and diversity in the generated text. A lower value will result in more focused and coherent text, while a higher value will lead to more diverse and potentially less coherent text. The default value is <code class="docutils literal notranslate"><span class="pre">5.0</span></code>.</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--mirostat</span> <span class="pre">2</span> <span class="pre">--mirostat_lr</span> <span class="pre">0.05</span> <span class="pre">--mirostat_ent</span> <span class="pre">3.0</span></code></p>
</section>
<section id="logit-bias">
<h2>Logit Bias<a class="headerlink" href="#logit-bias" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-l</span> <span class="pre">TOKEN_ID(+/-)BIAS,</span> <span class="pre">--logit-bias</span> <span class="pre">TOKEN_ID(+/-)BIAS</span></code>: Modify the likelihood of a token appearing in the generated text completion.</p></li>
</ul>
<p>The logit bias option allows you to manually adjust the likelihood of specific tokens appearing in the generated text. By providing a token ID and a positive or negative bias value, you can increase or decrease the probability of that token being generated.</p>
<p>For example, use <code class="docutils literal notranslate"><span class="pre">--logit-bias</span> <span class="pre">15043+1</span></code> to increase the likelihood of the token ‘Hello’, or <code class="docutils literal notranslate"><span class="pre">--logit-bias</span> <span class="pre">15043-1</span></code> to decrease its likelihood. Using a value of negative infinity, <code class="docutils literal notranslate"><span class="pre">--logit-bias</span> <span class="pre">15043-inf</span></code> ensures that the token <code class="docutils literal notranslate"><span class="pre">Hello</span></code> is never produced.</p>
<p>A more practical use case might be to prevent the generation of <code class="docutils literal notranslate"><span class="pre">\code{begin}</span></code> and <code class="docutils literal notranslate"><span class="pre">\code{end}</span></code> by setting the <code class="docutils literal notranslate"><span class="pre">\</span></code> token (29905) to negative infinity with <code class="docutils literal notranslate"><span class="pre">-l</span> <span class="pre">29905-inf</span></code>. (This is due to the prevalence of LaTeX codes that show up in LLaMA model inference.)</p>
<p>Example usage: <code class="docutils literal notranslate"><span class="pre">--logit-bias</span> <span class="pre">29905-inf</span></code></p>
</section>
<section id="rng-seed">
<h2>RNG Seed<a class="headerlink" href="#rng-seed" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-s</span> <span class="pre">SEED,</span> <span class="pre">--seed</span> <span class="pre">SEED</span></code>: Set the random number generator (RNG) seed (default: -1, &lt; 0 = random seed).</p></li>
</ul>
<p>The RNG seed is used to initialize the random number generator that influences the text generation process. By setting a specific seed value, you can obtain consistent and reproducible results across multiple runs with the same input and settings. This can be helpful for testing, debugging, or comparing the effects of different options on the generated text to see when they diverge. If the seed is set to a value less than 0, a random seed will be used, which will result in different outputs on each run.</p>
</section>
</section>
<section id="performance-tuning-and-memory-options">
<h1>Performance Tuning and Memory Options<a class="headerlink" href="#performance-tuning-and-memory-options" title="Permalink to this heading"></a></h1>
<p>These options help improve the performance and memory usage of the LLaMA models. By adjusting these settings, you can fine-tune the model’s behavior to better suit your system’s capabilities and achieve optimal performance for your specific use case.</p>
<section id="number-of-threads">
<h2>Number of Threads<a class="headerlink" href="#number-of-threads" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">N,</span> <span class="pre">--threads</span> <span class="pre">N</span></code>: Set the number of threads to use during computation. For optimal performance, it is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Using the correct number of threads can greatly improve performance.</p></li>
</ul>
</section>
<section id="mlock">
<h2>Mlock<a class="headerlink" href="#mlock" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--mlock</span></code>: Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM.</p></li>
</ul>
</section>
<section id="no-memory-mapping">
<h2>No Memory Mapping<a class="headerlink" href="#no-memory-mapping" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--no-mmap</span></code>: Do not memory-map the model. By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you’re not using <code class="docutils literal notranslate"><span class="pre">--mlock</span></code>. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all.</p></li>
</ul>
</section>
<section id="memory-float-32">
<h2>Memory Float 32<a class="headerlink" href="#memory-float-32" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--memory_f32</span></code>: Use 32-bit floats instead of 16-bit floats for memory key+value, allowing higher quality inference at the cost of higher memory usage.</p></li>
</ul>
</section>
<section id="batch-size">
<h2>Batch Size<a class="headerlink" href="#batch-size" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-b</span> <span class="pre">N,</span> <span class="pre">--batch_size</span> <span class="pre">N</span></code>: Set the batch size for prompt processing (default: 512). This large batch size benefits users who have BLAS installed and enabled it during the build. If you don’t have BLAS enabled (”BLAS=0”), you can use a smaller number, such as 8, to see the prompt progress as it’s evaluated in some situations.</p></li>
</ul>
</section>
<section id="prompt-caching">
<h2>Prompt Caching<a class="headerlink" href="#prompt-caching" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--prompt-cache</span> <span class="pre">FNAME</span></code>: Specify a file to cache the model state after the initial prompt. This can significantly speed up the startup time when you’re using longer prompts. The file is created during the first run and is reused and updated in subsequent runs.</p></li>
</ul>
</section>
<section id="quantization">
<h2>Quantization<a class="headerlink" href="#quantization" title="Permalink to this heading"></a></h2>
<p>For information about 4-bit quantization, which can significantly improve performance and reduce memory usage, please refer to llama.cpp’s primary <a class="reference external" href="../../README.md#prepare-data--run">README</a>.</p>
</section>
</section>
<section id="additional-options">
<h1>Additional Options<a class="headerlink" href="#additional-options" title="Permalink to this heading"></a></h1>
<p>These options provide extra functionality and customization when running the LLaMA models:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-h,</span> <span class="pre">--help</span></code>: Display a help message showing all available options and their default values. This is particularly useful for checking the latest options and default values, as they can change frequently, and the information in this document may become outdated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--verbose-prompt</span></code>: Print the prompt before generating text.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mtest</span></code>: Test the model’s functionality by running a series of tests to ensure it’s working properly.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--lora</span> <span class="pre">FNAME</span></code>: Apply a LoRA (Low-Rank Adaptation) adapter to the model (implies –no-mmap). This allows you to adapt the pretrained model to specific tasks or domains.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--lora-base</span> <span class="pre">FNAME</span></code>: Optional model to use as a base for the layers modified by the LoRA adapter. This flag is used in conjunction with the <code class="docutils literal notranslate"><span class="pre">--lora</span></code> flag, and specifies the base model for the adaptation.</p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>