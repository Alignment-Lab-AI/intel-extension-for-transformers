:orphan:

:py:mod:`intel_extension_for_transformers.evaluation.lm_eval.models.huggingface`
================================================================================

.. py:module:: intel_extension_for_transformers.evaluation.lm_eval.models.huggingface


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.evaluation.lm_eval.models.huggingface.AutoCausalLM
   intel_extension_for_transformers.evaluation.lm_eval.models.huggingface.AutoSeq2SeqLM
   intel_extension_for_transformers.evaluation.lm_eval.models.huggingface.MultiTokenEOSCriteria




.. py:class:: AutoCausalLM(pretrained: str, tokenizer: Optional[str] = None, subfolder: Optional[str] = None, revision: Optional[str] = 'main', batch_size: Optional[int] = 1, max_gen_toks: Optional[int] = 256, max_length: Optional[int] = None, add_special_tokens: Optional[bool] = None, use_accelerate: Optional[bool] = False, device_map_option: Optional[str] = 'auto', max_memory_per_gpu: Optional[Union[int, str]] = None, max_cpu_memory: Optional[Union[int, str]] = None, offload_folder: Optional[str] = './offload', dtype: Optional[Union[str, torch.dtype]] = None, device: Optional[Union[int, str]] = 'cuda')



   Causal language modeling.
   You can find a set of supported models in the HF documentation:
   https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForCausalLM


.. py:class:: AutoSeq2SeqLM(pretrained: str, tokenizer: Optional[str] = None, subfolder: Optional[str] = None, revision: Optional[str] = 'main', batch_size: Optional[int] = 1, max_gen_toks: Optional[int] = 256, max_length: Optional[int] = None, add_special_tokens: Optional[bool] = None, use_accelerate: Optional[bool] = False, device_map_option: Optional[str] = 'auto', max_memory_per_gpu: Optional[Union[int, str]] = None, max_cpu_memory: Optional[Union[int, str]] = None, offload_folder: Optional[str] = './offload', dtype: Optional[Union[str, torch.dtype]] = None, device: Optional[Union[int, str]] = 'cuda')



   Seq2Seq language modeling.
   You can find a set of supported models in the following documentation:
   https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForSeq2SeqLM


.. py:class:: MultiTokenEOSCriteria(sequence: str, tokenizer: transformers.PreTrainedTokenizer, initial_decoder_input_length: int, batch_size: int)



   Criteria to stop on the specified multi-token sequence.


