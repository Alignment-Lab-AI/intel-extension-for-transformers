:orphan:

:py:mod:`intel_extension_for_transformers.optimization`
=======================================================

.. py:module:: intel_extension_for_transformers.optimization


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   dynamic/index.rst
   mixture/index.rst
   utils/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   benchmark/index.rst
   config/index.rst
   model/index.rst
   optimizer/index.rst
   optimizer_tf/index.rst
   pipeline/index.rst
   pruning/index.rst
   quantization/index.rst
   trainer/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.optimization.AutoDistillationConfig
   intel_extension_for_transformers.optimization.DistillationConfig
   intel_extension_for_transformers.optimization.FlashDistillationConfig
   intel_extension_for_transformers.optimization.TFDistillationConfig
   intel_extension_for_transformers.optimization.Provider
   intel_extension_for_transformers.optimization.PruningConfig
   intel_extension_for_transformers.optimization.QuantizationConfig
   intel_extension_for_transformers.optimization.DynamicLengthConfig
   intel_extension_for_transformers.optimization.DistillationCriterionMode
   intel_extension_for_transformers.optimization.AutoDistillation
   intel_extension_for_transformers.optimization.OptimizedModel
   intel_extension_for_transformers.optimization.NoTrainerOptimizer
   intel_extension_for_transformers.optimization.Orchestrate_optimizer
   intel_extension_for_transformers.optimization.TFOptimization
   intel_extension_for_transformers.optimization.PrunerConfig
   intel_extension_for_transformers.optimization.PruningMode
   intel_extension_for_transformers.optimization.QuantizationMode




.. py:class:: AutoDistillationConfig(framework: str = 'pytorch', search_space: dict = {}, search_algorithm: str = 'BO', metrics: Union[List, intel_extension_for_transformers.optimization.utils.metrics.Metric] = None, max_trials: int = None, seed: int = None, knowledge_transfer: FlashDistillationConfig = None, regular_distillation: FlashDistillationConfig = None)

   Bases: :py:obj:`object`

   Configure the auto disillation process.

   .. py:property:: knowledge_transfer

      Get the knowledge transfer.

   .. py:property:: regular_distillation

      Get the regular distillation.

   .. py:property:: framework

      Get the framework.

   .. py:property:: search_space

      Get the search space.

   .. py:property:: search_algorithm

      Get the search algorithm.

   .. py:property:: max_trials

      Get the max trials.

   .. py:property:: seed

      Get the seed.

   .. py:property:: metrics

      Get the metrics.


.. py:class:: DistillationConfig(framework: str = 'pytorch', criterion: intel_extension_for_transformers.optimization.distillation.Criterion = None, metrics: intel_extension_for_transformers.optimization.utils.metrics.Metric = None, inc_config=None)

   Bases: :py:obj:`object`

   Configure the distillation process.

   .. py:property:: framework

      Get the framework.

   .. py:property:: criterion

      Get the criterion.

   .. py:property:: metrics

      Get the metrics.


.. py:class:: FlashDistillationConfig(block_names: list = [], layer_mappings_for_knowledge_transfer: list = [], loss_types: list = [], loss_weights: list = [], add_origin_loss: list = [], train_steps: list = [])

   Bases: :py:obj:`object`

   The flash distillation configuration used by AutoDistillationConfig.


.. py:class:: TFDistillationConfig(loss_types: list = [], loss_weights: list = [], train_steps: list = [], temperature: float = 1.0)

   Bases: :py:obj:`object`

   Configure the distillation process for Tensorflow.


.. py:class:: Provider

   Bases: :py:obj:`enum.Enum`

   Optimization functionalities provider: INC or NNCF.


.. py:class:: PruningConfig(framework: str = 'pytorch', epochs: int = 1, epoch_range: List = [0, 4], initial_sparsity_ratio: float = 0.0, target_sparsity_ratio: float = 0.97, metrics: intel_extension_for_transformers.optimization.utils.metrics.Metric = None, pruner_config: Union[List, neural_compressor.conf.config.Pruner] = None, config_file: str = None)

   Bases: :py:obj:`object`

   Configure the pruning process.

   .. py:property:: pruner_config

      Get the pruner config.

   .. py:property:: target_sparsity_ratio

      Get the target sparsity ratio.

   .. py:property:: initial_sparsity_ratio

      Get the initial sparsity ratio.

   .. py:property:: epoch_range

      Get the epoch range.

   .. py:property:: epochs

      Get the epochs.

   .. py:property:: framework

      Get the framework.

   .. py:property:: metrics

      Get the metrics.

   .. py:method:: init_prune_config()

      Init the pruning config.



.. py:class:: QuantizationConfig(framework: str = 'pytorch', approach: str = 'PostTrainingStatic', timeout: int = 0, max_trials: int = 100, metrics: Union[intel_extension_for_transformers.optimization.utils.metrics.Metric, List] = None, objectives: Union[intel_extension_for_transformers.optimization.utils.objectives.Objective, List] = performance, config_file: str = None, sampling_size: int = 100, use_bf16: bool = False)

   Bases: :py:obj:`object`

   Configure the quantization process.

   .. py:property:: approach

      Get the quantization approach.

   .. py:property:: input_names

      Get the input names.

   .. py:property:: output_names

      Get the output names.

   .. py:property:: metrics

      Get the metrics.

   .. py:property:: framework

      Get the framework.

   .. py:property:: objectives

      Get the objectives.

   .. py:property:: strategy

      Get the strategy.

   .. py:property:: timeout

      Get the timeout.

   .. py:property:: op_wise

      Get the op_wise dict.

   .. py:property:: max_trials

      Get the number of maximum trials.

   .. py:property:: performance_only

      Get the boolean whether to use performance only.

   .. py:property:: random_seed

      Get the random seed.

   .. py:property:: tensorboard

      Get the boolean whether to use tensorboard.

   .. py:property:: output_dir

      Get the output directory.

   .. py:property:: resume_path

      Get the resume path.

   .. py:property:: sampling_size

      Get the sampling size.


.. py:class:: DynamicLengthConfig(max_length: int = None, length_config: str = None, const_rate: float = None, num_sandwich: int = 2, length_drop_ratio_bound: float = 0.2, layer_dropout_prob: float = None, layer_dropout_bound: int = 0, dynamic_training: bool = False, load_store_file: str = None, evo_iter: int = 30, population_size: int = 20, mutation_size: int = 30, mutation_prob: float = 0.5, crossover_size: int = 30, num_cpus: int = 48, distributed_world_size: int = 5, latency_constraint: bool = True, evo_eval_metric='eval_f1')

   Bases: :py:obj:`object`

   Configure the dynamic length config for Quantized Length Adaptive Transformer.


.. py:class:: DistillationCriterionMode

   Bases: :py:obj:`enum.Enum`

   Generic enumeration.

   Derive from this class to define new enumerations.


.. py:class:: AutoDistillation(model_builder, conf_fname_or_obj, framework='pytorch')

   Bases: :py:obj:`object`

   The framework class is designed for handling the whole pipeline of AutoDistillation.

   AutoDistillation is composed of three major stages, i.e. Model Exploration, Flash Distillation,
   and Evaluation.
   In Model Exploration, a search engine will search for a better compressed model from the architecture
   design space in each iteration.
   Flash Distillation is the stage for training the searched model to discover its potential.
   In Evaluation stage, the trained model will be evaluated to measure its performances (e.g.
   the prediction accuracy, the hardware performance etc.) in order to select the best model architecture.

   .. py:property:: teacher_model

      Getter of teacher model.

   .. py:property:: student_model

      Getter of student model.

   .. py:property:: advisor

      Getter of advisor.

   .. py:property:: train_func

      Getter of train function.

   .. py:property:: eval_func

      Getter of evaluation function.

   .. py:method:: model_arch_proposition()

      Propose architecture of the model based on search algorithm for next search iteration.

      :returns: Model architecture description.


   .. py:method:: search(res_save_path=None, model_cls=None)

      Auto distillation search process.

      :returns: Best model architecture found in search process.


   .. py:method:: estimate(model)

      Train and evaluate the model.

      :returns: Evaluated metrics of the model.


   .. py:method:: load_search_results(path)

      Load previous search results.

      :param path: The file path which stores the previous results.


   .. py:method:: dump_search_results(path)

      Dump current search results into a file.

      :param path: The file path to store the results.


   .. py:method:: params_vec2params_dict(paras_vec)

      Transfer the vector into dict to hold the paramaters.


   .. py:method:: find_best_model_archs()

      Find the model architecture with best performance.


   .. py:method:: metrics_conversion(metrics)

      Convert the metrics.


   .. py:method:: init_by_cfg(conf_fname_or_obj)

      Use auto distillation config to init the instance of autodistillation.


   .. py:method:: init_search_cfg(config)

      Init advisor base on config.


   .. py:method:: create_distillers()

      Create flash and regular distillers.



.. py:class:: OptimizedModel(*args, **kwargs)

   Provide the from_pretrained function.

   .. py:method:: from_pretrained(model_name_or_path: str, **kwargs)
      :classmethod:

      Instantiate a quantized pytorch model from a given Intel Neural Compressor (INC) configuration file.

      :param model_name_or_path: Repository name in the Hugging Face Hub or path to a local directory hosting the model.
      :type model_name_or_path: :obj:`str`
      :param cache_dir: Path to a directory in which a downloaded configuration should be cached if the standard cache should
                        not be used.
      :type cache_dir: :obj:`str`, `optional`
      :param force_download: Whether or not to force to (re-)download the configuration files and override the cached versions if
                             they exist.
      :type force_download: :obj:`bool`, `optional`, defaults to :obj:`False`
      :param resume_download: Whether or not to delete incompletely received file. Attempts to resume the download if such a file
                              exists.
      :type resume_download: :obj:`bool`, `optional`, defaults to :obj:`False`
      :param revision: The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                       git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any
                       identifier allowed by git.
      :type revision: :obj:`str`, `optional`

      :returns: Quantized model.
      :rtype: q_model



.. py:class:: NoTrainerOptimizer(model, output_dir: Optional[str] = 'saved_results')

   Optimizer without using Trainer.

   .. py:property:: eval_func

      Get the evaluation function.

   .. py:property:: train_func

      Get the train function.

   .. py:property:: calib_func

      Get the calib function.

   .. py:property:: provider

      Get the provider.

   .. py:property:: calib_dataloader

      Get the calibration dataloader.

   .. py:method:: init_quantizer(quant_config, provider: str = Provider.INC.value)

      Init a Quantization object with config.


   .. py:method:: quantize(quant_config: intel_extension_for_transformers.optimization.QuantizationConfig = None, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None, calib_func: Optional[Callable] = None, calib_dataloader=None)

      Prepare for invoking the _inc_quantize function.


   .. py:method:: init_pruner(pruning_config=None, provider: str = Provider.INC.value)

      Init a Pruning object with config.


   .. py:method:: prune(pruning_config=None, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      Do the pruning.


   .. py:method:: init_distiller(distillation_config, teacher_model, provider: str = Provider.INC.value)

      Init a Distillation object with config and the teacher model.


   .. py:method:: distill(distillation_config, teacher_model, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      Do the distillation.



.. py:class:: Orchestrate_optimizer(model, components: Optional[List[neural_compressor.experimental.Component]] = [], eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

   Orchestrate_optimizer aggregates and orchestrates components such as Quantization, Pruning and Distillation.

   .. py:method:: fit()

      Run the scheduler.



.. py:class:: TFOptimization(model: transformers.PreTrainedModel, args: transformers.training_args_tf.TFTrainingArguments, train_dataset=None, eval_dataset=None, compute_metrics: Optional[Callable] = None, criterion=None, optimizer=None, task_type=None, task_id=None, strategy=None)

   TFOptimization is the entry class for Tensorflow to use the optimization techniques in neural compressor.

   .. py:property:: inputs

      Get the inputs.

   .. py:property:: input_names

      Get the input names.

   .. py:property:: output_names

      Get the output names.

   .. py:property:: eval_func

      Get the evaluation function.

   .. py:property:: train_func

      Get the training function.

   .. py:property:: train_dataset

      Get the training dataset.

   .. py:property:: eval_dataset

      Get the evaluation dataset.

   .. py:method:: builtin_eval_func(model)

      Customize Evaluate function to inference the model for specified metric on the validation dataset.

      :param model: The model will be the class of tf.saved_model.load(quantized_model_path).
      :type model: [tf.saved_model.load]

      :returns: evaluation result, the larger is better.
      :rtype: [float]


   .. py:method:: init_quantizer(quant_config)

      Init a Quantization object with config.


   .. py:method:: quantize(quant_config: intel_extension_for_transformers.optimization.QuantizationConfig = None, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None, train_dataset=None, eval_dataset=None)

      Prepare for invoking INC quantize function.


   .. py:method:: init_pruner(pruning_config=None)

      Init a Pruning object with config.


   .. py:method:: prune(pruning_config=None, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None, train_dataset=None, eval_dataset=None)

      Do the pruning.


   .. py:method:: init_distiller(distillation_config, teacher_model: transformers.PreTrainedModel)

      Init a Distillation object with config and the teacher model.


   .. py:method:: distill(distillation_config, teacher_model: transformers.PreTrainedModel, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      Do the distillation.


   .. py:method:: model_builder_builtin(arch_paras=None, model_cls=None)

      Specify model_cls to use the built-in model builder.


   .. py:method:: autodistill(autodistillation_config, teacher_model: transformers.PreTrainedModel, model_builder: Optional[Callable] = None, model_cls: Optional[Callable] = None, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      Do the auto distillation.


   .. py:method:: build_train_func(model)

      Build the training function for pruning or distillation.



.. py:class:: PrunerConfig(epoch_range: List = [0, 4], initial_sparsity_ratio: float = 0.0, target_sparsity_ratio: float = 0.97, update_frequency: int = 1, prune_type: str = 'BasicMagnitude', method: str = 'per_tensor', names: List = [], parameters: Dict = None)

   Bases: :py:obj:`neural_compressor.conf.config.Pruner`

   Pruner configuration.


.. py:class:: PruningMode

   Bases: :py:obj:`enum.Enum`

   Currently support three pruning modes.


.. py:class:: QuantizationMode

   Bases: :py:obj:`enum.Enum`

   Currently support three quantization modes.


