:py:mod:`intel_extension_for_transformers.optimization.pipeline`
================================================================

.. py:module:: intel_extension_for_transformers.optimization.pipeline

.. autoapi-nested-parse::

   Pipeline: import transformers.pipelines and support int8 model loading based on infer_framework_load_model.



Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.optimization.pipeline.infer_framework_load_model



.. py:function:: infer_framework_load_model(model, config: transformers.AutoConfig, model_classes: Optional[Dict[str, Tuple[type]]] = None, task: Optional[str] = None, framework: Optional[str] = None, **model_kwargs)

   Support int8 model loading based on infer_framework_load_model.

   :returns: A tuple framework, model.
   :rtype: `Tuple`


