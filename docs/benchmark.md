# Benchmark
The Benchmark is inherited from transformers [Benchmark]
(https://github.com/huggingface/transformers/blob/main/docs/source/en/benchmarks.mdx). Right now, The classes `PyTorchBenchmark` and `ExecutorBenchmark` can help users to check the performance benefit and the model size benefit of FP32 and INT8 model on PyTorch and Executor backends.

The benchmark classes [PyTorchBenchmark] and [ExecutorBenchmark] expect an object of type [PyTorchBenchmarkArguments] and [ExecutorBenchmarkArguments].

----

## PyTorchBenchmark

The PyTorchBenchmark is only used for inference when the input model is an INT8 model.

```py
from intel_extension_for_transformers.optimization.benchmark import PyTorchBenchmark, PyTorchBenchmarkArguments

MODEL_ID_FP32 = "distilbert-base-uncased-finetuned-sst-2-english"
MODEL_ID_INT8 = "Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static"
benchmark_args = PyTorchBenchmarkArguments(
    models=[MODEL_ID_FP32, MODEL_ID_INT8],
    training=False,
    memory=False,
    inference=True,
    sequence_lengths=[32, 64, 128],
    batch_sizes=[1],
    multi_process=False,
    only_pretrain_model=True,
)
benchmark = PyTorchBenchmark(benchmark_args)
results = benchmark.run()
```

```py
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
          Model Name             Batch Size     Seq Length     Time in s   
--------------------------------------------------------------------------------
distilbert-base-uncased-finetu       1               32            0.017     
distilbert-base-uncased-finetu       1               64            0.021     
distilbert-base-uncased-finetu       1              128            0.029     
Intel/distilbert-base-uncased-       1               32             0.01     
Intel/distilbert-base-uncased-       1               64            0.013     
Intel/distilbert-base-uncased-       1              128            0.017     
--------------------------------------------------------------------------------

====================    INFERENCE - MODEL SIZE - RESULT     ====================
--------------------------------------------------------------------------------
                         Model Name                         Model Size in MB
--------------------------------------------------------------------------------
      distilbert-base-uncased-finetuned-sst-2-english             255.45    
     Intel/distilbert-base-uncased-finetuned-sst-2-engl           65.026    
--------------------------------------------------------------------------------
```

---

## ExecutorBenchmark

The ExecutorBenchmark is only used for inference. The ONNX model is generated by our `export_to_onnx` API. For more details, please [go to](export.md). The MODEL_NAME is the pytorch model name you used for exporting the ONNX model.

```py
from intel_extension_for_transformers.optimization.benchmark import ExecutorBenchmark, ExecutorBenchmarkArguments
from transformers import AutoConfig

MODEL_ID_FP32 = 'fp32.onnx'
MODEL_ID_INT8 = 'int8.onnx'
config = AutoConfig.from_pretrained(MODEL_NAME)
benchmark_args = ExecutorBenchmarkArguments(
    models=[MODEL_ID_FP32, MODEL_ID_INT8],
    memory=False,
    inference=True,
    sequence_lengths=[32, 64, 128],
    batch_sizes=[1],
    multi_process=False,
    only_pretrain_model=True,
)
benchmark = ExecutorBenchmark(benchmark_args, configs=[config, config])
results = benchmark.run()
```

```py
====================       INFERENCE - SPEED - RESULT       ====================
--------------------------------------------------------------------------------
          Model Name             Batch Size     Seq Length     Time in s   
--------------------------------------------------------------------------------
          fp32.onnx                  1               32            0.008     
          fp32.onnx                  1               64            0.012     
          fp32.onnx                  1              128            0.023     
          int8.onnx                  1               32            0.003     
          int8.onnx                  1               64            0.004     
          int8.onnx                  1              128            0.008     
--------------------------------------------------------------------------------

====================    INFERENCE - MODEL SIZE - RESULT     ====================
--------------------------------------------------------------------------------
                         Model Name                         Model Size in MB
--------------------------------------------------------------------------------
                         fp32.onnx                               255.466    
                         int8.onnx                                132.53    
--------------------------------------------------------------------------------
```
